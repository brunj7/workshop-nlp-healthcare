# Exploratory data analysis
Having now imported the data as a TF-IDF matrix, we aree able to begin with some exploratory data analysis. The analysis can help us to understand patterns in the big stack of abstracts about breast cancer. It won't tell us much about any particular one of the abstracts - remember that we've reduced each one to a vector of term frequencies so as a text document each abstract has been stripped of meaning. But we are going to be able to identify large-scale patterns in the collection of abstracts much more quickly than if we had to read each one.

## Principal components analysis
Principal components analysis, which will be abbreviated PCA, is a method used in exploratory data analysis for multidimensional data. That's what we're working with, since the 8180 columns of the TF-IDF matrix are more than we could possibly explore on a column-by-column basis.

### Check the TF-IDF matrix
Not only are there too many columns to work through individually, but also each column is mostly zeroes. As a result an individual column of the TF-IDF matrix doesn't contain much information. This makes sense if you consider consider how much you would learn about books published in 2020 by counting the number of times each one used ther word "whale". Here's an example. 

```{r plot-tfidf}
# first, convert the TF-IDF to a dense matrix
tfmat = as.matrix( tfidf_dtm )

# identify the first term and plot its histogram
hist( tfmat[, 1], main=paste0( 'uses of "', colnames( tfmat )[[1]], '"' ))

# identify the second term and plot its histgram
hist( tfmat[, 2], main=paste0( 'uses of "', colnames( tfmat )[[2]], '"' ))

# plot the first two columns 
plot( as.matrix(tfidf_dtm[, 1:2]), bty='n', main=paste0( 'joint uses of "',
  colnames( tfmat )[[1]], '" and "', colnames( tfmat )[[2]], '"') )
```

### Compute PCA 
Since the individual columns don't convey much information, PCA rotates the big matrix until we are "looking" down the "direction" (or component) with the most variability. The result is another matrix of equal size to the first, but each column contains information from all of the columns of the original matrix, and they are sorted so that the columns with the most variability are first. This will take your computer several seconds to calculate.

```{r pca-calc}
# rotate the TF-IDF so the columns are articles
articles = t( tfmat )

# calculate PCA on the rotated TF-IDF
pca = prcomp(articles, center=TRUE, scale=TRUE)

# extract the matrix of rotations
pcmat = as.data.frame( pca$rotation )
```

### Plot the abstracts
We can now look at a plot of the first two principal components to see how it contains more information than the first columns of the TF-IDF matrix.

```{r}
# plot the first two principal components
with( pcmat, plot(PC1, PC2) )
```


Unlike the TF-IDF, the x and y axes here have no inherent meaning, but they are aligned with the greatest variability in the data. So by inspecting the articles that are at the extremes, we can begin to understand the strongest signals that PCA detected in the articles.

### Examine how principal components sort the documents


```{r}
# identify the order of documents along the first principal component
indx <- order( pcmat$PC1 )

# view the titles at the extremes of the first principal component
data$title[ head(indx) ]
data$title[ tail(indx) ]
```

It seems that the PCA has detected a pattern in term usage that distinguishes between papers on one extreme that make comparisons between whole- and partial-breast radiation therapy, and papers on the other extreme that are mostly about molecular biology. This is kind of impressive when you consider that the computer did it without any human input about what language might be relevant to medicine or biology. Let's look at how some other principal components are separating the documents.

```{r second-pc}
# identify the order of documents along the second principal component
indx = order( pcmat$PC2 )

# observe which papers are at the extremes of the second PC
data$title[ head(indx) ]
data$title[ tail(indx) ]
```

With the second principal component, we see a separation between papers about breast-conserving therapy at one extreme and papers about hypofractionated, conformal, or modulated radiation therapy at the other extreme.

```{r third-pc}
# identify the order of documents along the third principal component
indx = order( pcmat$PC3 )

# observe which papers are at the extremes of the second PC
data$title[ head(indx) ]
data$title[ tail(indx) ]
```

At one extreme of the third principal component are articles about intraoperative radiation therapy and at the other extreme are articles about radiation dosimetry, especially with respect to modulated and conformal radiation therapy.

### Interpreting PCA
It looks like there are patterns of actual meaning arising from the PCA. We're not talking here about statistical significance or clinical importance. It is still for humans to decide what patterns have clinical importance. Here, PCA is a shortcut that allows us to identify some patterns in articles about breast cancer radiation therapy without having to read all of the articles. All of the patterns I've cited are based on patterns of word usage, and yet it appears that those patterns in word usage actually map onto distinctions with real-world meaning. So keep in mind that the principal components are only identifying patterns in term frequencies but the PCA is useful only when those terms map onto real-world meaning.

So far, we've looked at the most extreme documents along some principal components. If you want to see more than a few documents at a time, or see how documents align jointly on two principal components, then you want to do a scatterplot of the top terms of each document. I'll show you how, but it uses a function called `top_terms()` that needs to be loaded from our Github repository (it's too complex to live-code).

```{r}
# load the function for identifying top terms in documents
source(url("https://ucdavisdatalab.github.io/workshop-nlp-healthcare/top_terms.R"))

# cbind data and the PCA rotation
plotdata <- cbind(data, pcmat)

# identify top terms for each document and attach them to the plotdata
plotdata[[ 'top_terms' ]] <- top_terms( tfidf_dtm )

# visualize the top principal components in terms of top terms
ggplot(plotdata) + aes(x=PC2, y=PC3, label=top_terms) + geom_text(check_overlap=TRUE)
```

We've seen the patterns that align with the first, second, and third principal components. What makes these principal components the first, second, and third? The PCs are ranked by the amount of total variance in the TF-IDF matrix that they explain. So the first principal component is aligned with the strongest signal in term frequency, and the second is aligned with the strongest signal that's left over after accounting for the first component, and so on. The strength of signal is quantified by the proportion of variance in the TF-IDF matrix that is explained by each principal component. In order to see how the strength of signal changes along the sequence of PCs, we can visualize the cumulative percent of variance explained.

```{r}
plot(100 * cumsum(pca$sdev^2) / sum(pca$sdev^2), type='l', bty='n',
    ylab="% total variance explained", xlab="Number of components")
```

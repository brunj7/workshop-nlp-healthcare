---
title: NLP Workshop
author: Dr. Pamela Reynolds, Arthur Koehl, Dr. Wesley Brooks
date: February 10, 2020
output:
   rmdformats::readthedown
---

# Before we get started

link to lesson plan: [https://ucdavisdatalab.github.io/workshop-nlp-healthcare/](https://ucdavisdatalab.github.io/workshop-nlp-healthcare/)

If you will be coding along please check that you have R and RStudio installed on your machine.


# Introductions

#### a. About us

#### b. About you 

[Word frequencies](wordfrequencies.html)
[Bigrams](bigrams.html)

# Objectives for this Workshop
``` {objective}
1. Explain natural language processing in lay terms
2. Give examples of NLP applications for research
3. Evaluate particular challenges posed by working with health data
4. Describe key text mining and NLP metrics for assessing word importance and document similarity
    - Install the 'tm' package in R 
    - Create a corpus and DocumentTermMatrix
    - Calculate word frequencies
    - Calculate TF-IDF weighting 
    - Generate PCA for exploring document similarities
5. identify where to go to learn more!
```

# Intro to NLP

There is *a lot* of health data that is unstrctured text. 

* EMR
* Clinical notes
* Medical journal literature
* Surveys and questionnaires
* Interviews
* Community forums
* Social media

NLP allows us to use "distance reading" to unlock information from narrative text for extraction and classification:

* keyword detection
* topic detection
* document summarizing
* document classification
* document clustering
* document similarity
* speech recognition
* text translation

*Natural Language Processing* (NLP) = linguistics + computer science + information engineering + data science. We use NLP to computationally parse and analyze large amounts of natural language data.

NLP takes **text to numbers and numbers to machines.**

#### What's a Natural Language?

"...any language that has evolved naturally in humans through use and repetition without conscious planning or premeditation. Natural languages can take different forms, such as speech or signing. They are distinguished from constructed and formal languages such as those used to program computers or to study logic." [Thanks, Wikipedia.](https://en.wikipedia.org/wiki/Natural_language)

**Thought questions:** Are clinical notes a natural language? What about tweets?

#### History of NLP

1950s - Alan Turing's article on "Computing Machinery and Intelligence"
1954 - Georgetown Experiment to automatically translate 60 Russian sentences to English
1960s - Restricted vocabularies
1970s - Conceptual ontologies and chatterbots
1980s - First statistical machine translation systems developed (IBM, Watson)
2010s - Extensions of classical NLP and emergence of deep neural networks

In this workshop we're focusing on classical NLP (feature engineering). 

#### Caveats before we get started

**Garbage in, garbage out.** Know your corpus.
**All models are wrong, but some are useful.** And others are dangerous.

*Natural langauge data can not be deidentified.*

NLP models are powerful, but can fail when applied to jargon-heavy, niched domains. Augmenting with developed medical dictionaries helps, but parts of speech taggers, for example, are not designed for the health space. This is especially true for classical NLP techniques, although deep learning approaches hold strong promise for working with unstructured health data.

#### Text processing
    - *word frequency*:
    - *TF-IDF*:
    - *PCA*:
    - word dispersion:
    - KWIC: Key Word in Context
    - co-occurence:
    - stemming:
    - bigrams: conditional probability of a token given the preceding token.
    - named entity recognition (NER): identifying which items in the text map to proper names (people, places, location).
    - regular expressions (RegEx): searching for exact words, parts of words or phrases
    - feature extraction:
    - document clustering (e.g kmeans)
    - parsing: grammatical analysis of a sentence and relationships between words.
    - feature based linear classifiers:
    - topic modeling:
    - word embedding: mapping words or phrases to vectors of real numbers allowing words with similar meaning to have a similar representation.
    - sentiment analysis: subjective information to determine "polarity" of a document. (e.g., positive or negative reviews)
    - lemmatization - removing inflectional endings to return the base dictionary form of a word (lemma).
    - part of speech tagging: determining the parts of speech for each word in a sentence.
    - dependency parsing:
    - OCR: optical character recognition, determining the text from an image of the text.

# Where to go to learn more

* DataLab's NLP Researcher Toolkit]()
* [Curated list of ML and NLP resources for healthcare](https://github.com/isaacmg/healthcare_ml)
* [Post on how NLP can help clinicians](https://tdan.com/natural-language-processing-in-healthcare/24538)
* [Machine Learning for Healthcare August 2020 Conference at Duke](https://www.mlforhc.org/)
* NLP Healthcare Conference

# Demo introduction

Dr. Sonja Diertrich is a radiation oncologist at UCDMC studying early stage breast cancer. For this coding demo we are using NLP to explore the recent literature on this topic with her lab. 

#### About the data

Working with  Blaisdell Medical Library's health librarian Amy Studer, we conducted a pilot systematic search in EMBASE and PubMed on this topic for articles published in the International Journal of Radiation Oncology Biology Physics, Practical Radiation Oncology, and Lancet over the past 5 years. For this demo we are using the titles and abstracts from that search. (See the readME for more information regarding the search strategy used to generate our datatset.) 

#### Why R? What is RStudio?

["R"](https://www.r-project.org/) is a programming language and statistical software that interprets scripts. [RStudio](https://rstudio.com/) is an Integrated Development Environment (IDE) for working with R. You need R installed in order to use RStudio, but you don't need RStudio to work with R.

*So why R?*
* Faciliates reproducible research workflows
    - integrates with other tools. 
    - designed for data analysis
    - produces high-quality graphics
* It's extensible and interdisciplinary
    - free, open-source, well-documented, and cross-platform
    - runs on Mac, PC and Linux. 
    - works with data of all shapes and sizes.
    - large library of external packages available for performing diverse tasks, including text mining and NLP.
* Large (and growing) community
 
*Should you use R?* Maybe. Use whatever langauge allows you to perform your work and faciliates sharing with your direct colleagues and the broader reserach community. 

# Get your stickies ready

# Setup


#### a. objectives
``` {objective}
1. Show how to store and clean text data with the 'tm' package
2. Introduce Bag of Words representation of documents
3. Demonstrate how to create and use a DocumentTermMatrix
4. Explain and demonstrate TF-IDF weighting 
5. Explain PCA and its applications for corpus exploration
```

#### b. packages needed

In R, and most programming languages, there are many packages - code written by other people to help with certain tasks. 
For this workshop we will be using two packages - 'tm' and 'ggplot2'.
The 'tm' - text mining - package has methods for mining text with R including importing data, storing corpora, applying operations on corpora (such as common preprocessing methods), and document term matrices.
The 'ggplot2' package has functions related to plotting and visualizing data.


Run this command if you don't already have these packages installed.
```{r, eval=FALSE}
install.packages('tm')
install.packages('ggplot2')
install.packages('Matrix')
```

Once the packages are installed, load them into your R environment.
```{r, message=FALSE}
library('tm')
library('ggplot2')
library('Matrix')
```

You can find the documentation for the package [online](https://cran.r-project.org/web/packages/tm/tm.pdf)\
Within R/RStudio you can browse function documentation with the following syntax.
```
?TermDocumentMatrix
```

#### c. data for this workshop

For this workshop, we are looking at a set of abstracts of medical journal articles related to breast cancer.
We have 714 abstracts, stored in a csv, with duplicates. 
We would like to get the key words from each abstract, as well as visualize / check for groupings of abstracts in two dimensions.\


```{r}
data <- read.csv(url("https://ucdavisdatalab.github.io/workshop-nlp-healthcare/abstracts.csv"), stringsAsFactors=FALSE)
```

```{r}
head(data)
```
We have a dataframe with 714 rows, each row referring to a different abstract. 
For each abstract we have the authors, year published, title of the paper, name of the journal, and the full text.

Let's look at the text from the first abstract
```{r}
data$text[[1]]
```
Notice that within the text there are a variety of potential issues. 
For example, some words are capitalized, there is punctuation, weird symbols, and numbers. 
For many NLP methods, we want to normalize the texts to get around these issues. 
The 'tm' package has several built in features for normalizing text. 
The first step is to load the text into a 'corpus' object.


# Preprocessing

#### a. Load the text column into a 'corpus' object
```{r}
mycorpus <- Corpus(VectorSource(data$text))
inspect(head(mycorpus))
```

#### b.  Preprocess the corpus object
Use the tm_map function to apply a transformation on each element of the corpus object.  
Alternatively use the tm_parLapply function to do the same in parallel.
```{r, warning=FALSE}
mycorpus <- tm_map(mycorpus, tolower)
mycorpus <- tm_map(mycorpus, removePunctuation, ucp=TRUE)
mycorpus <- tm_map(mycorpus, removeNumbers)
mycorpus <- tm_map(mycorpus, removeWords, stopwords("en")) 
```

Now that we have normalized the text, lets look at the first abstract again.
```{r}
mycorpus[[1]]$content
```

It looks 'normalized' but how do we model this? how do we apply NLP algorithms on it?

# The Bag of Words Representation

Consider: \
what is a text document to a computer?  \
What can it do with a sequence of characters? \  

In order for us to apply statistical methods on a document, we need a representation of texts that is easy for a computer to process, but still encodes information related to that text's content.
One such representation is the Bag of Words format.\

Bag of Words is a way of representing a document that encodes a document as a 'bag' of its tokens.
The document is represented as the words that appeared in the document and the number of times those words appeared.
All information about word order is lost in this representation, however, for many NLP methods, this is still an effective representation of the content of the document.\

![bag of words image](./img/bow.png)

The power of the bag of words representation is that each document can be represented in the same vector space.
We do so by defining the vector dimensions to reflect the vocabulary across all the documents.
The vectors can then be merged into a matrix called a Document Term Matrix.

![dtm image](./img/dtm.png)


# The Document Term Matrix

In brief, a Document Term Matrix:  
  - each document is represented by a set of tokens and their counts  
  - the order of tokens is not encoded in this representation  
  - the basis of many text processing methods, including document classification and topic modeling  

In R we can use a DocumentTermMatrix function from the 'tm' package to create this structure from our corpus.  

#### a. Creating a Document Term Matrix from the corpus object    
From the 'corpus' object we can create a document term matrix.
```{r}
mydtm <- DocumentTermMatrix(mycorpus)
```
Note: the DocumentTermMatrix automatically sets all the characters to lower case.

#### b. Exploring with a DTM  

A useful tool is the inspect function from the 'tm' package.
```{r}
inspect(mydtm)
```

From this format it we can find word counts and document lengths.
```{r}
document_lengths <- rowSums(as.matrix(mydtm))
word_counts <- colSums(as.matrix(mydtm))
```

We can get a sorted list of the biggest documents.
```{r}
sorted_document_lengths <- sort(document_lengths, decreasing=TRUE)
barplot(sorted_document_lengths[1:10], col = "tan", las = 2)
```
Or a sorted list of words and their frequencies.
```{r}
sorted_word_counts <- sort(word_counts, decreasing=TRUE)
barplot(sorted_word_counts[1:10], col = "tan", las = 2)
```

# TF-IDF

TF-IDF stands for term frequency-inverse document frequency. 
It is a VERY popular method for finding documents relevant to a users search term. 
It can also be used as an effective (often times better than simple bag of words) representation of documents for statistical modeling of documents in a corpus. 

#### a. Intuition behind TF-IDF

TF-IDF combines two attributes that may signal a words importance in a document into a single metric. 
The first is the 'term frequency (TF)' - how often the word appeared within that document. 
It makes intuitive sense that if a word appears many times in a document, that the document is about something related to that term.
The second attribute is the 'inverse document frequency (IDF)' - a measure of what proportion of the documents the word appeared in. If a word appears in all documents, its weight should be reduced. 
Conversely, if a word appears only in few documents, it should be highly weighted for those documents. 

#### b. TF-IDF formula

```
tfidf(t,d,D) = tf(t,d) * idf(t,D)
```
Where, tf(t,d) is a function of a terms(t) frequency for a given document(d). 
And, idf(t,D) is the inverse function of a terms(t) appearance across all the documents(D).  There are many variations of functions for tf(t,d) and idf(t,D) that can be used for computing TF-IDF.  


#### c. TF-IDF from DTM
Create tf-idf weighted DTM in R
```{r}
tfidf_dtm <- weightTfIdf(mydtm, normalize=TRUE) 
```

#### d. Looking at the results
Inspect the new dtm
```{r}
inspect(tfidf_dtm)
```

Compare the tfidf representation with the tf representation for a single abstract
```{r}
tf_doc10 <- as.matrix(mydtm[10,])
tf <- colSums(tf_doc10)
tfidf_doc10 <- as.matrix(tfidf_dtm[10,])
tfidf <- colSums(tfidf_doc10)
barplot(sort(tf, decreasing=TRUE)[1:10], col = "tan", las = 2)
barplot(sort(tfidf, decreasing=TRUE)[1:10], col = "tan", las = 2)
```

# PCA

#### a. Intuition behind PCA
The most straightforward way to visualize the information in the TF-IDF matrix is to plot its columns, but this is unsatisfactory because there are thousands of them, and each column is mostly zeroes (so contains little information):

```{r}
plot(as.matrix(tfidf_dtm[, 1:2]))
```

Our answer is to use principal components analysis (PCA), which rotates the big matrix until we are "looking" down the "direction" (or component) with the most variability. The result is another matrix of equal size to the first, but each column contains information from all of the columns of the original matrix, and they are sorted so that the most important columns are first. We can observe get the most important relationships by focusing on just a few of these "principal components". Let's continue with our example:

#### b. Computing PCA 

```{r}
articles <- t(tfidf_dtm)
mypca <- prcomp(articles, center=TRUE, scale=TRUE)
```

#### c. Plotting the abstracts

Add the principal components to the original data:

```{r}
plotdata <- cbind(data, mypca$rotation)
```

And now plot the abstracts. Each dot in the figure represents one abstract.

```{r}
ggplot(plotdata) + aes(x=PC1, y=PC2) + geom_point()
```

Now let's identify some articles from the plot. 

```{r}
min_index <- order(plotdata$PC1, decreasing=FALSE)
max_index <- order(plotdata$PC1, decreasing=TRUE)

# view the most divergent titles:
plotdata$title[min_index[1:6]]
plotdata$title[max_index[1:6]]
```

Visualize the explained variance. This is the metric that PCA uses to decide which components are the most important.

```{r}
plot(100 * cumsum(mypca$sdev^2) / sum(mypca$sdev^2), type='l', bty='n', ylab="% total variance explained", xlab="Number of components")
```

Let's add some additional information: a few top terms from each abstract. These are the most characeristic terms from the abstract, as ranked by TF-IDF. We'll plot the abstracts on the 2nd and 3rd principal components, annotated by the top terms.

```{r}
source(url("https://ucdavisdatalab.github.io/workshop-nlp-healthcare/top_terms.R"))
plotdata[['top_terms']] <- top_terms(tfidf_dtm)
ggplot(plotdata) + aes(x=PC2, y=PC3, label=top_terms) + geom_text(check_overlap=TRUE)
```

We're also going to look at k-means to identify clusters in data. To begin, let's include some abstracts dealing with cervical cancer from the same journals as the breast cancer abstracts.

```{r import-cervical-abstracts}
cervical <- read.csv(url("https://ucdavisdatalab.github.io/workshop-nlp-healthcare/cervical-abstracts.csv"))
data[["topic"]] <- "breast"
cervical[["topic"]] <- "cervical"
combined <- rbind(data, cervical)
```

Rather than processing the combined abstracts step-by-step, let's grab a function to replicate the processing we did on the breast cancer abstracts. This time, though, the terms "breast" "cervical", and "cervix" will be removed prior to the analysis.

```{r, warning=FALSE}
source(url("https://ucdavisdatalab.github.io/workshop-nlp-healthcare/get_corpus.R"))
pca_combined <- get_corpus(combined, remove=c("cervical", "cervix", "breast"))
data_combined <- cbind(combined, pca_combined$rotation)
```

We now have a PCA of the combined abstracts, so let's see how the first few components look:

```{r}
ggplot(data_combined) + aes(x=PC1, y=PC2, color=topic) + geom_point()
ggplot(data_combined) + aes(x=PC2, y=PC3, color=topic) + geom_point()
```

If we didn't know the correct labels, we can let the computer pick them itself. One commonly-used method that's built into R is k-means classification. K-means identifies clusters in data based on maximizing the similarity within groups while maximizing the dissimilarity between groups. It is often best to use PCA for dimension-reduction before applying PCA because separation between points increases with the square of the number of dimensions, and that separation makes clustering less effective. Here I will be quite aggressive and use only three principal components.

```{r k-means}
km <- kmeans(pca_combined$rotation[, 1:3], 2, iter.max=1e3, nstart=100)
km_fit <- fitted(km, method="classes")
data_combined[["group_fit"]] <- as.factor(km_fit)

ggplot(data_combined) + aes(x=PC2, y=PC3, color=group_fit) + geom_point()
```

Another way to explore the result of k-means is to look at the articles that are most characteristic of each cluster. First, calculate the sum of squared residuals for each article.

```{r}
km_residual <- pca_combined$rotation[, 1:3] - fitted(km)
km_ss_residual <- apply(km_residual, 1, var)
data_combined[["err"]] <- km_ss_residual
```

Now we can identify the articles within each group that have the smallest residuals. These are closest to the group center, and thus most characteristic of the group.

```{r}
data_ordered <- data_combined[order(data_combined[["err"]]),]
head(data_ordered[["title"]][data_ordered[["group_fit"]] == 1])
head(data_ordered[["title"]][data_ordered[["group_fit"]] == 2])
```

# Further resources 

Useful packages:
- ntlk (python)
- spaCy (python)
- quanteda (R)
- tidytext (R)

Classes:
- [Dan Jurafsky and Christopher Manning NLP youtube series](https://www.youtube.com/playlist?list=PLoROMvodv4rOFZnDyrlW3-nI7tMLtmiJZ&disable_polymer=true)

Datasets:
- [https://github.com/niderhoff/nlp-datasets](https://github.com/niderhoff/nlp-datasets)
- [https://archive.ics.uci.edu/ml/datasets.php?format=&task=&att=&area=&numAtt=&numIns=&type=text&sort=nameUp&view=table](https://archive.ics.uci.edu/ml/datasets.php?format=&task=&att=&area=&numAtt=&numIns=&type=text&sort=nameUp&view=table)
